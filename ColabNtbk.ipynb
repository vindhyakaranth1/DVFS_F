{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Google Borg Cluster Trace Processing for AI-Based CPU Scheduler\n",
        "================================================================\n",
        "This notebook processes Borg trace data to train a Linear Regression model\n",
        "that predicts the next CPU burst time based on the history of the last 3 bursts.\n",
        "\n",
        "Author: AI Scheduler Project\n",
        "Dataset: Google Borg Cluster Trace (2019) - Kaggle version\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Setup and Imports\n",
        "# ============================================================================\n",
        "print(\"Installing required packages...\")\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import joblib\n",
        "from google.colab import drive, files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Imports successful\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Mount Google Drive\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: Mounting Google Drive\")\n",
        "print(\"=\"*70)\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Google Drive mounted successfully\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Load CSV Data\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 3: Loading Borg Trace Data\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "file_path = '/content/borg_traces_data.csv'\n",
        "print(f\"Reading from: {file_path}\")\n",
        "print(\"Loading first 1,000,000 rows...\")\n",
        "\n",
        "try:\n",
        "    # Load data with robust error handling (pandas >= 1.3)\n",
        "    df = pd.read_csv(\n",
        "        file_path,\n",
        "        nrows=1000000,\n",
        "        on_bad_lines='skip'\n",
        "    )\n",
        "    print(f\"✓ Loaded {len(df):,} rows successfully\")\n",
        "    print(f\"✓ Columns: {list(df.columns)}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading CSV: {e}\")\n",
        "    print(\"\\nTrying alternative loading method...\")\n",
        "    try:\n",
        "        # Try with C engine and low_memory\n",
        "        df = pd.read_csv(\n",
        "            file_path,\n",
        "            nrows=1000000,\n",
        "            low_memory=False\n",
        "        )\n",
        "    except Exception as e2:\n",
        "        print(f\"✗ Second attempt failed: {e2}\")\n",
        "        print(\"\\nTrying basic loading...\")\n",
        "        # Fallback to simplest approach\n",
        "        df = pd.read_csv(file_path, nrows=1000000)\n",
        "\n",
        "    print(f\"✓ Loaded {len(df):,} rows with alternative method\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Robust Feature Extraction\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 4: Extracting CPU Burst Durations\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Keep only necessary columns\n",
        "required_cols = ['collection_id', 'instance_index', 'time']\n",
        "df = df[required_cols].copy()\n",
        "\n",
        "print(f\"Working with {len(df):,} rows\")\n",
        "print(\"Sorting by task and time...\")\n",
        "\n",
        "# Sort by collection_id, instance_index, and time\n",
        "df = df.sort_values(['collection_id', 'instance_index', 'time']).reset_index(drop=True)\n",
        "\n",
        "print(\"Calculating time deltas between consecutive events...\")\n",
        "\n",
        "# Calculate duration as time difference between consecutive logs for the same task\n",
        "df['prev_time'] = df.groupby(['collection_id', 'instance_index'])['time'].shift(1)\n",
        "df['duration'] = df['time'] - df['prev_time']\n",
        "\n",
        "# Filter out invalid durations\n",
        "print(\"Filtering valid durations...\")\n",
        "valid_durations = df[\n",
        "    (df['duration'].notna()) &\n",
        "    (df['duration'] > 0) &\n",
        "    (df['duration'] < 1e15)  # Remove extremely large outliers\n",
        "].copy()\n",
        "\n",
        "print(f\"✓ Extracted {len(valid_durations):,} valid burst durations\")\n",
        "print(f\"\\nDuration statistics (raw):\")\n",
        "print(valid_durations['duration'].describe())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Scaling & Normalization\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: Scaling and Normalizing Durations\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Apply log transform to handle wide variance\n",
        "print(\"Applying log transform...\")\n",
        "valid_durations['log_duration'] = np.log1p(valid_durations['duration'])\n",
        "\n",
        "print(\"Normalizing to 1-100 range (simulating milliseconds)...\")\n",
        "# Normalize to 1-100 range\n",
        "min_log = valid_durations['log_duration'].min()\n",
        "max_log = valid_durations['log_duration'].max()\n",
        "valid_durations['normalized_duration'] = (\n",
        "    1 + 99 * (valid_durations['log_duration'] - min_log) / (max_log - min_log)\n",
        ")\n",
        "valid_durations['normalized_duration'] = valid_durations['normalized_duration'].round().astype(int)\n",
        "\n",
        "print(f\"\\n✓ Normalized duration statistics:\")\n",
        "print(valid_durations['normalized_duration'].describe())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Create Training Sequences\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: Creating Training Sequences\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Flatten into a stream of bursts\n",
        "burst_stream = valid_durations['normalized_duration'].values\n",
        "\n",
        "print(f\"Total bursts in stream: {len(burst_stream):,}\")\n",
        "\n",
        "# Create sliding windows: [T-3, T-2, T-1] -> [T]\n",
        "X = []  # Input features (last 3 bursts)\n",
        "y = []  # Target (next burst)\n",
        "\n",
        "window_size = 3\n",
        "print(f\"Creating sliding windows (window_size={window_size})...\")\n",
        "\n",
        "for i in range(len(burst_stream) - window_size):\n",
        "    X.append(burst_stream[i:i+window_size])\n",
        "    y.append(burst_stream[i+window_size])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"✓ Created {len(X):,} training samples\")\n",
        "print(f\"✓ Input shape: {X.shape}\")\n",
        "print(f\"✓ Target shape: {y.shape}\")\n",
        "\n",
        "print(f\"\\nSample training data:\")\n",
        "print(f\"First 5 samples:\")\n",
        "for i in range(min(5, len(X))):\n",
        "    print(f\"  Input: {X[i]} -> Target: {y[i]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Train Linear Regression Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 7: Training Linear Regression Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Training set: {len(X_train):,} samples\")\n",
        "print(f\"Test set: {len(X_test):,} samples\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining Linear Regression model...\")\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"✓ Model trained successfully\")\n",
        "print(f\"\\nModel coefficients: {model.coef_}\")\n",
        "print(f\"Model intercept: {model.intercept_:.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Evaluate Model\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 8: Model Evaluation\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "# Calculate errors\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE):\")\n",
        "print(f\"  Training set: {mae_train:.2f} ms\")\n",
        "print(f\"  Test set:     {mae_test:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSample predictions vs actual (Test set):\")\n",
        "for i in range(min(10, len(X_test))):\n",
        "    print(f\"  Input: {X_test[i]} -> Predicted: {y_pred_test[i]:.1f} ms, Actual: {y_test[i]} ms\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: Save Model and Download\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 9: Saving and Downloading Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save model\n",
        "model_filename = 'burst_predictor.pkl'\n",
        "print(f\"Saving model to {model_filename}...\")\n",
        "joblib.dump(model, model_filename)\n",
        "print(\"✓ Model saved successfully\")\n",
        "\n",
        "# Save model metadata\n",
        "metadata = {\n",
        "    'model_type': 'LinearRegression',\n",
        "    'window_size': window_size,\n",
        "    'num_training_samples': len(X_train),\n",
        "    'num_test_samples': len(X_test),\n",
        "    'mae_train': mae_train,\n",
        "    'mae_test': mae_test,\n",
        "    'coefficients': model.coef_.tolist(),\n",
        "    'intercept': float(model.intercept_),\n",
        "    'min_log': float(min_log),\n",
        "    'max_log': float(max_log),\n",
        "    'normalization_range': [1, 100]\n",
        "}\n",
        "\n",
        "metadata_filename = 'model_metadata.txt'\n",
        "with open(metadata_filename, 'w') as f:\n",
        "    for key, value in metadata.items():\n",
        "        f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "print(f\"✓ Metadata saved to {metadata_filename}\")\n",
        "\n",
        "# Trigger download\n",
        "print(\"\\nDownloading files to your local machine...\")\n",
        "files.download(model_filename)\n",
        "files.download(metadata_filename)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓✓✓ PIPELINE COMPLETE ✓✓✓\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  - Processed {len(df):,} raw events\")\n",
        "print(f\"  - Extracted {len(valid_durations):,} valid bursts\")\n",
        "print(f\"  - Created {len(X):,} training samples\")\n",
        "print(f\"  - Model MAE: {mae_test:.2f} ms\")\n",
        "print(f\"  - Model saved as: {model_filename}\")\n",
        "print(\"\\nYou can now use this model in your SJF scheduler simulator!\")\n",
        "\n",
        "# ============================================================================\n",
        "# BONUS: Example Usage Function\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BONUS: Example Model Usage\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def predict_next_burst(last_three_bursts):\n",
        "    \"\"\"\n",
        "    Predict the next CPU burst time given the last 3 bursts.\n",
        "\n",
        "    Args:\n",
        "        last_three_bursts: List/array of 3 integers (1-100 range)\n",
        "\n",
        "    Returns:\n",
        "        Predicted next burst time (float)\n",
        "    \"\"\"\n",
        "    input_data = np.array(last_three_bursts).reshape(1, -1)\n",
        "    prediction = model.predict(input_data)[0]\n",
        "    return max(1, min(100, prediction))  # Clamp to valid range\n",
        "\n",
        "# Example predictions\n",
        "print(\"\\nExample predictions:\")\n",
        "examples = [\n",
        "    [10, 15, 20],\n",
        "    [50, 45, 40],\n",
        "    [80, 85, 90],\n",
        "    [25, 30, 35]\n",
        "]\n",
        "\n",
        "for example in examples:\n",
        "    pred = predict_next_burst(example)\n",
        "    print(f\"  Last 3 bursts: {example} -> Predicted next: {pred:.1f} ms\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Script execution completed successfully!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tLJAPvp7_Msh",
        "outputId": "a4318d28-9016-47f6-f553-7f5cbbe3b2f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "✓ Imports successful\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Mounting Google Drive\n",
            "======================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ Google Drive mounted successfully\n",
            "\n",
            "======================================================================\n",
            "STEP 3: Loading Borg Trace Data\n",
            "======================================================================\n",
            "Reading from: /content/borg_traces_data.csv\n",
            "Loading first 1,000,000 rows...\n",
            "✓ Loaded 405,894 rows successfully\n",
            "✓ Columns: ['Unnamed: 0', 'time', 'instance_events_type', 'collection_id', 'scheduling_class', 'collection_type', 'priority', 'alloc_collection_id', 'instance_index', 'machine_id', 'resource_request', 'constraint', 'collections_events_type', 'user', 'collection_name', 'collection_logical_name', 'start_after_collection_ids', 'vertical_scaling', 'scheduler', 'start_time', 'end_time', 'average_usage', 'maximum_usage', 'random_sample_usage', 'assigned_memory', 'page_cache_memory', 'cycles_per_instruction', 'memory_accesses_per_instruction', 'sample_rate', 'cpu_usage_distribution', 'tail_cpu_usage_distribution', 'cluster', 'event', 'failed']\n",
            "\n",
            "First few rows:\n",
            "   Unnamed: 0           time  instance_events_type  collection_id  \\\n",
            "0           0              0                     2    94591244395   \n",
            "1           1  2517305308183                     2   260697606809   \n",
            "2           2   195684022913                     6   276227177776   \n",
            "3           3              0                     2    10507389885   \n",
            "4           4  1810627494172                     3    25911621841   \n",
            "\n",
            "   scheduling_class  collection_type  priority  alloc_collection_id  \\\n",
            "0                 3                1       200                    0   \n",
            "1                 2                0       360         221495397286   \n",
            "2                 2                0       103                    0   \n",
            "3                 3                0       200                    0   \n",
            "4                 2                0         0                    0   \n",
            "\n",
            "   instance_index    machine_id  ... assigned_memory page_cache_memory  \\\n",
            "0             144  168846390496  ...        0.014435          0.000415   \n",
            "1             335      85515092  ...        0.000000          0.000000   \n",
            "2             376  169321752432  ...        0.010422          0.000235   \n",
            "3            1977  178294817221  ...        0.041626          0.000225   \n",
            "4            3907  231364893292  ...        0.000272          0.000010   \n",
            "\n",
            "   cycles_per_instruction memory_accesses_per_instruction sample_rate  \\\n",
            "0                     NaN                             NaN         1.0   \n",
            "1                     NaN                             NaN         1.0   \n",
            "2                0.939919                        0.001318         1.0   \n",
            "3                1.359102                        0.007643         1.0   \n",
            "4                     NaN                             NaN         1.0   \n",
            "\n",
            "                              cpu_usage_distribution  \\\n",
            "0  [0.00314331 0.00381088 0.00401306 0.00415039 0...   \n",
            "1  [1.23977661e-05 1.23977661e-05 1.23977661e-05 ...   \n",
            "2  [0.01344299 0.01809692 0.0201416  0.02246094 0...   \n",
            "3  [0.03704834 0.04125977 0.04290771 0.04425049 0...   \n",
            "4  [0.         0.         0.         0.         0...   \n",
            "\n",
            "                         tail_cpu_usage_distribution  cluster     event  \\\n",
            "0  [0.00535583 0.00541687 0.00548553 0.00554657 0...        7      FAIL   \n",
            "1  [1.23977661e-05 1.23977661e-05 1.23977661e-05 ...        7      FAIL   \n",
            "2  [0.02902222 0.02929688 0.0295105  0.0296936  0...        7  SCHEDULE   \n",
            "3  [0.05535889 0.05584717 0.05633545 0.05718994 0...        8      FAIL   \n",
            "4  [0.00041485 0.00041485 0.00041485 0.00041485 0...        2    FINISH   \n",
            "\n",
            "   failed  \n",
            "0       1  \n",
            "1       1  \n",
            "2       0  \n",
            "3       1  \n",
            "4       0  \n",
            "\n",
            "[5 rows x 34 columns]\n",
            "\n",
            "======================================================================\n",
            "STEP 4: Extracting CPU Burst Durations\n",
            "======================================================================\n",
            "Working with 405,894 rows\n",
            "Sorting by task and time...\n",
            "Calculating time deltas between consecutive events...\n",
            "Filtering valid durations...\n",
            "✓ Extracted 159,850 valid burst durations\n",
            "\n",
            "Duration statistics (raw):\n",
            "count    1.598500e+05\n",
            "mean     1.500649e+11\n",
            "std      3.568878e+11\n",
            "min      2.000000e+00\n",
            "25%      9.586045e+08\n",
            "50%      7.805804e+09\n",
            "75%      9.038133e+10\n",
            "max      2.675915e+12\n",
            "Name: duration, dtype: float64\n",
            "\n",
            "======================================================================\n",
            "STEP 5: Scaling and Normalizing Durations\n",
            "======================================================================\n",
            "Applying log transform...\n",
            "Normalizing to 1-100 range (simulating milliseconds)...\n",
            "\n",
            "✓ Normalized duration statistics:\n",
            "count    159850.000000\n",
            "mean         76.577441\n",
            "std          19.370812\n",
            "min           1.000000\n",
            "25%          71.000000\n",
            "50%          79.000000\n",
            "75%          88.000000\n",
            "max         100.000000\n",
            "Name: normalized_duration, dtype: float64\n",
            "\n",
            "======================================================================\n",
            "STEP 6: Creating Training Sequences\n",
            "======================================================================\n",
            "Total bursts in stream: 159,850\n",
            "Creating sliding windows (window_size=3)...\n",
            "✓ Created 159,847 training samples\n",
            "✓ Input shape: (159847, 3)\n",
            "✓ Target shape: (159847,)\n",
            "\n",
            "Sample training data:\n",
            "First 5 samples:\n",
            "  Input: [88  1 98] -> Target: 100\n",
            "  Input: [  1  98 100] -> Target: 100\n",
            "  Input: [ 98 100 100] -> Target: 89\n",
            "  Input: [100 100  89] -> Target: 96\n",
            "  Input: [100  89  96] -> Target: 93\n",
            "\n",
            "======================================================================\n",
            "STEP 7: Training Linear Regression Model\n",
            "======================================================================\n",
            "Training set: 127,877 samples\n",
            "Test set: 31,970 samples\n",
            "\n",
            "Training Linear Regression model...\n",
            "✓ Model trained successfully\n",
            "\n",
            "Model coefficients: [0.2465526  0.32138098 0.05075359]\n",
            "Model intercept: 29.21\n",
            "\n",
            "======================================================================\n",
            "STEP 8: Model Evaluation\n",
            "======================================================================\n",
            "Mean Absolute Error (MAE):\n",
            "  Training set: 10.10 ms\n",
            "  Test set:     5.68 ms\n",
            "\n",
            "Sample predictions vs actual (Test set):\n",
            "  Input: [67 67 67] -> Predicted: 70.7 ms, Actual: 70 ms\n",
            "  Input: [67 67 70] -> Predicted: 70.8 ms, Actual: 67 ms\n",
            "  Input: [67 70 67] -> Predicted: 71.6 ms, Actual: 71 ms\n",
            "  Input: [70 67 71] -> Predicted: 71.6 ms, Actual: 68 ms\n",
            "  Input: [67 71 68] -> Predicted: 72.0 ms, Actual: 68 ms\n",
            "  Input: [71 68 68] -> Predicted: 72.0 ms, Actual: 70 ms\n",
            "  Input: [68 68 70] -> Predicted: 71.4 ms, Actual: 67 ms\n",
            "  Input: [68 70 67] -> Predicted: 71.9 ms, Actual: 72 ms\n",
            "  Input: [70 67 72] -> Predicted: 71.7 ms, Actual: 68 ms\n",
            "  Input: [67 72 68] -> Predicted: 72.3 ms, Actual: 68 ms\n",
            "\n",
            "======================================================================\n",
            "STEP 9: Saving and Downloading Model\n",
            "======================================================================\n",
            "Saving model to burst_predictor.pkl...\n",
            "✓ Model saved successfully\n",
            "✓ Metadata saved to model_metadata.txt\n",
            "\n",
            "Downloading files to your local machine...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e1dc27a5-cf1f-4397-8858-9325e363323a\", \"burst_predictor.pkl\", 593)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e2903036-ff3a-4bd0-863a-7e9f323cb495\", \"model_metadata.txt\", 347)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "✓✓✓ PIPELINE COMPLETE ✓✓✓\n",
            "======================================================================\n",
            "\n",
            "Summary:\n",
            "  - Processed 405,894 raw events\n",
            "  - Extracted 159,850 valid bursts\n",
            "  - Created 159,847 training samples\n",
            "  - Model MAE: 5.68 ms\n",
            "  - Model saved as: burst_predictor.pkl\n",
            "\n",
            "You can now use this model in your SJF scheduler simulator!\n",
            "\n",
            "======================================================================\n",
            "BONUS: Example Model Usage\n",
            "======================================================================\n",
            "\n",
            "Example predictions:\n",
            "  Last 3 bursts: [10, 15, 20] -> Predicted next: 37.5 ms\n",
            "  Last 3 bursts: [50, 45, 40] -> Predicted next: 58.0 ms\n",
            "  Last 3 bursts: [80, 85, 90] -> Predicted next: 80.8 ms\n",
            "  Last 3 bursts: [25, 30, 35] -> Predicted next: 46.8 ms\n",
            "\n",
            "======================================================================\n",
            "Script execution completed successfully!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}